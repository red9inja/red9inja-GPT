# Model configuration for large GPT
model_size: large

# Training
batch_size: 8
epochs: 30
learning_rate: 1e-4
weight_decay: 0.1
grad_clip: 1.0

# Optimizer
optimizer: adamw
betas: [0.9, 0.95]

# Scheduler
scheduler: cosine
warmup_steps: 5000

# Data
max_seq_len: 2048
data_path: data/train.txt
val_split: 0.1

# Logging
log_interval: 50
save_interval: 2
checkpoint_dir: checkpoints

# Device
device: cuda
mixed_precision: true
gradient_accumulation_steps: 8

# Distributed training
distributed: true
world_size: 4
