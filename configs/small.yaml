# Model configuration for small GPT
model_size: small

# Training
batch_size: 32
epochs: 10
learning_rate: 3e-4
weight_decay: 0.1
grad_clip: 1.0

# Optimizer
optimizer: adamw
betas: [0.9, 0.95]

# Scheduler
scheduler: cosine
warmup_steps: 1000

# Data
max_seq_len: 512
data_path: data/train.txt
val_split: 0.1

# Logging
log_interval: 100
save_interval: 5
checkpoint_dir: checkpoints

# Device
device: cuda  # or cpu
mixed_precision: true
