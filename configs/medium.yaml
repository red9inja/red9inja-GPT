# Model configuration for medium GPT
model_size: medium

# Training
batch_size: 16
epochs: 20
learning_rate: 2e-4
weight_decay: 0.1
grad_clip: 1.0

# Optimizer
optimizer: adamw
betas: [0.9, 0.95]

# Scheduler
scheduler: cosine
warmup_steps: 2000

# Data
max_seq_len: 1024
data_path: data/train.txt
val_split: 0.1

# Logging
log_interval: 100
save_interval: 5
checkpoint_dir: checkpoints

# Device
device: cuda
mixed_precision: true
gradient_accumulation_steps: 4
